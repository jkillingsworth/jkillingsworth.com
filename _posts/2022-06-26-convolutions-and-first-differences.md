---
layout: post
title: Convolutions and First Differences
---

This is a study of the convolution operation and its applications to time series data and probability distributions. In this post, I first demonstrate the use of the convolution operation to find the first differences of some randomly generated time series data. I then show how to find the distribution of the first differences based on the distribution of the values in the original series. I also show how to work backwards using deconvolution, which is the inverse of the convolution operation.

<!--excerpt-->

## Times Series Data

Suppose we generate a sequence of random numbers at equally spaced time intervals. The random numbers are integers ranging from negative two to positive two. This is our time series data. Let's use the following notation to represent them:

{% latex fig-01 %}
    \begin{document}
    \begin{displaymath}
    x(t) = X_t, \quad \forall t \in \{\, 0, \dots, N \,\}
    \end{displaymath}
    \end{document}
{% endlatex %}

Using a random number generator, we can create a sample data set to work with. If we plot these numbers on a chart, this is what it looks like:

{% chart fig-02-example-1-series-xs.svg %}

Now we want to find the first difference for this time series. That is to say, we want to compute the difference from one data point to the next, creating a new time series in the process. Here is a formula we can use to do this:

{% latex fig-03 %}
    \begin{document}
    \begin{displaymath}
    y(t) = x(t) - x(t - 1), \quad \forall t \in \{\, 1, \dots, N \,\}
    \end{displaymath}
    \end{document}
{% endlatex %}

The first differencing operation requires a sequential pair of input data points to produce a single output data point. Because of this, the output data set will always contain one less data point than the input data set. Since our input data starts at time zero, our output data starts at time one. It's simple enough to just compute the first differences using this formula. But there is an alternative method we can explore. Consider the following:

{% latex fig-04 %}
    \begin{document}
    \begin{displaymath}
    h(t) =
    \begin{dcases}
    +1              & \quad \text{if $t = 0$}
    \\
    -1              & \quad \text{if $t = 1$}
    \\
    \phantom{\pm} 0 & \quad \text{otherwise}
    \end{dcases}
    \end{displaymath}
    \end{document}
{% endlatex %}

You can think of this as a filtering operation or an impulse response function. If we treat our time series data as a signal, we can use convolution as a general-purpose mechanism to apply various filters to the data---including the one above that computes first differences. Here is how to apply the filtering operation using convolution:

{% latex fig-05 %}
    \begin{document}
    \begin{displaymath}
    y(t) = (x * h)(t) = \sum_{k = -\infty}^{\infty} x(k) \0 h(t - k)
    \end{displaymath}
    \end{document}
{% endlatex %}

In theory, the convolution operation is a summation across an infinite range of values. Any function value that is not explicitly defined is just treated as a zero. In practice, however, it is only necessary to compute the sum across the range of values that actually exist:

{% latex fig-06 %}
    \begin{document}
    \begin{displaymath}
    y(t) = \sum_{k = 0}^{N} x(k) \0 h(t - k)
    \end{displaymath}
    \end{document}
{% endlatex %}

Evaluating this function for each point in time within the valid range, we can obtain a time series consisting of the first differences of the original input data. Here is a visual representation of the first differences:

{% chart fig-07-example-1-series-ys.svg %}

I think it's worth mentioning here that the convolution operation is commutative. That means the order in which you place the operands doesn't matter:

{% latex fig-08 %}
    \begin{document}
    \begin{displaymath}
    (x * h) = (h * x)
    \end{displaymath}
    \end{document}
{% endlatex %}

One of the interesting consequences of this is that you can rearrange the convolution formula so that it requires fewer operations to compute. Consider the following rearrangement:

{% latex fig-09 %}
    \begin{document}
    \begin{displaymath}
    y(t) = \sum_{k = 0}^{1} h(k) \0 x(t - k)
    \end{displaymath}
    \end{document}
{% endlatex %}

This yields the same results as before. But notice that the limits of the summation go from zero to one instead of needlessly summing across the entire span of the input data. Arranging it this way can have a significant performance benefit when dealing with large data sets.

## Discrete Probability Distribution

In the example above, the time series data was produced by generating a sequence of random integers between negative two and positive two. These numbers were generated by taking samples of a random variable with the following probability mass function:

{% latex fig-10 %}
    \begin{document}
    \begin{displaymath}
    f(x) =
    \begin{dcases}
    0.10 & \quad \text{if $x = +2$}
    \\
    0.20 & \quad \text{if $x = +1$}
    \\
    0.40 & \quad \text{if $x = \phantom{\pm} 0$}
    \\
    0.20 & \quad \text{if $x = -1$}
    \\
    0.10 & \quad \text{if $x = -2$}
    \\
    0.00 & \quad \text{otherwise}
    \end{dcases}
    \end{displaymath}
    \end{document}
{% endlatex %}

Here is a visual representation of this function:

{% chart fig-11-example-2-pmfunc-fs.svg %}

If this mass function represents the distribution of the input time series data, what is the distribution of the first differences? That is the question we want to answer. And to answer this question, we can use the convolution operation. Again. Only this time around, we take the convolution of the probability mass function with itself:

{% latex fig-12 %}
    \begin{document}
    \begin{displaymath}
    g(x) = (f * f)(x) = \sum_{k = -\infty}^{\infty} f(k) \0 f(x - k)
    \end{displaymath}
    \end{document}
{% endlatex %}

Like before, the limits of the summation need not span an infinite range of values for practical implementations. The convolution operation defined above gives us a new probability mass function that represents the distribution of the first differences. If you work it out, here is the result:

{% latex fig-13 %}
    \begin{document}
    \begin{displaymath}
    g(x) =
    \begin{dcases}
    0.01 & \quad \text{if $x = +4$}
    \\
    0.04 & \quad \text{if $x = +3$}
    \\
    0.12 & \quad \text{if $x = +2$}
    \\
    0.20 & \quad \text{if $x = +1$}
    \\
    0.26 & \quad \text{if $x = \phantom{\pm} 0$}
    \\
    0.20 & \quad \text{if $x = -1$}
    \\
    0.12 & \quad \text{if $x = -2$}
    \\
    0.04 & \quad \text{if $x = -3$}
    \\
    0.01 & \quad \text{if $x = -4$}
    \\
    0.00 & \quad \text{otherwise}
    \end{dcases}
    \end{displaymath}
    \end{document}
{% endlatex %}

Here is a visual representation of this function:

{% chart fig-14-example-2-pmfunc-gs.svg %}

As you can see, the distribution of first differences is more dispersed and has a wider range of possible outcomes. If the result of this convolution operation is not intuitive to you, then I would recommend that you stop here and work this example out manually on a piece of paper. That will help you understand it better.

## Continuous Normal Distribution

What if the sequence of random numbers used to generate the time series is distributed according to a continuous probability distribution? How can we determine the distribution of the first differences in this case? For this example, suppose the random numbers are normally distributed. This can be represented by the following probability density function:

{% latex fig-15 %}
    \begin{document}
    \begin{displaymath}
    f(x)
    =
    \frac{1}{\sigma \sqrt{2 \pi}} \exp \2 \brace4[]{- \frac{(x - \mu)^2}{2 \sigma^2}}
    \end{displaymath}
    \end{document}
{% endlatex %}

Here is a visual representation of this function in standard form using a mean value of zero and a standard deviation value of one:

{% chart fig-16-example-3-pdfunc-fs.svg %}

To find the distribution of the first differences, we can use the convolution operation in the same manner as we did in the previous example. But instead of a sum, we use an integral:

{% latex fig-17 %}
    \begin{document}
    \begin{displaymath}
    g(x) = (f * f)(x) = \int_{-\infty}^{\infty} f(\tau) \0 f(x - \tau) \, \dderiv \tau
    \end{displaymath}
    \end{document}
{% endlatex %}

This might be a difficult integral to solve. It might not have a closed-form solution. And you might have to resort to numerical methods to find an approximate solution. Using numerical integration, we can easily create a plot to see what this function looks like:

{% chart fig-18-example-3-pdfunc-gs.svg %}

At first glance, this looks like a more dispersed version of the standard normal distribution that we started with. And indeed, that's what it is. I went ahead and worked out the integral. Here is the solution I came up with:

{% latex fig-19 %}
    \begin{document}
    \begin{displaymath}
    g(x)
    =
    \frac{1}{2 \sigma \sqrt{\pi}} \exp \2 \brace4[]{- \frac{(x - 2 \mu)^2}{4 \sigma^2}}
    \end{displaymath}
    \end{document}
{% endlatex %}

This takes the form of a normal distribution. Applying the convolution operation effectively doubles the mean and doubles the variance while maintaining the shape of the distribution. The shape of the distribution is maintained in this example because it's a normal distribution. This outcome may not hold for other types of probability distributions.

## Continuous Laplace Distribution

I am curious to see what happens if the distribution of the values in the initial time series data is a Laplace distribution instead of a normal distribution. What will the shape of the distribution of the first differences look like? Here is the density function of the Laplace distribution:

{% latex fig-20 %}
    \begin{document}
    \begin{displaymath}
    f(x)
    =
    \frac{1}{2b} \exp \2 \brace3(){- \frac{|x - \mu|}{b}}
    \end{displaymath}
    \end{document}
{% endlatex %}

Here is a visual representation of this function using a location parameter value of zero and a scale parameter value of one:

{% chart fig-21-example-4-pdfunc-fs.svg %}

We can apply the convolution operation the same way we did in the previous example. And then, using numerical integration, we can create a plot to see what the density function looks like for the first differences:

{% chart fig-22-example-4-pdfunc-gs.svg %}

The shape of this density function looks like a cross between that of a Laplace distribution and that of a normal distribution. I wasn't able to work out the integral analytically. But my suspicion is that it takes the form of a [generalized normal distribution](https://en.wikipedia.org/wiki/Generalized_normal_distribution) with a shape parameter somewhere between one and two. I'll leave this as an unsolved problem for now.

## Deconvolution

Now let's say we know the distribution of the first differences. How do we work backwards to compute the distribution of the values in the original time series based on our knowledge of the distribution of the first differences? For this example, let's assume the first differences are distributed according to a Laplace distribution. Here is the density function:

{% latex fig-23 %}
    \begin{document}
    \begin{displaymath}
    g(x)
    =
    \frac{1}{2b} \exp \2 \brace3(){- \frac{|x - \mu|}{b}}
    \end{displaymath}
    \end{document}
{% endlatex %}

Here is a visual representation of this function using a location parameter value of zero and a scale parameter value of two:

{% chart fig-24-example-5-pdfunc-gs.svg %}

Given this function, can we find the inverse of the self-convolution operation demonstrated in the two previous examples? One approach we can use to answer this question is based on the [convolution theorem](https://en.wikipedia.org/wiki/Convolution_theorem). The convolution theorem states that the Fourier transform of the convolution of two functions is the product of the Fourier transform of each of the two functions individually. To phrase this another way, we can say that convolution on the primary domain is equivalent to multiplication on the frequency domain:

{% latex fig-25 %}
    \begin{document}
    \begin{displaymath}
    g = (f * f)
    \quad \Longleftrightarrow \quad
    G = F \cdot F
    \end{displaymath}
    \end{document}
{% endlatex %}

Taking this relationship into consideration, we can see that the Fourier transform of the distribution of the time series data---the function we are trying to solve for---is equal to the square root of the Fourier transform of the distribution of the first differences:

{% latex fig-26 %}
    \begin{document}
    \begin{displaymath}
    F(\omega) = \sqrt{G(\omega)}
    \end{displaymath}
    \end{document}
{% endlatex %}

We know the distribution of the first differences is a Laplace distribution. We can find the Fourier transform of the density function using the following formula:

{% latex fig-27 %}
    \begin{document}
    \begin{displaymath}
    G(\omega)
    =
    \mathcal{F}^{\,+1\!} \brace2{\lbrace}{\rbrace}{ g(x) }
    =
    \int_{-\infty}^{\infty} g(x) \, e^{-i \omega x} \, \dderiv x
    \end{displaymath}
    \end{document}
{% endlatex %}

The Fourier transform of the Laplace density function works out to this:

{% latex fig-28 %}
    \begin{document}
    \begin{displaymath}
    G(\omega)
    =
    \frac{e^{- i \mu \omega}}{b^2 \omega^2 + 1}
    \end{displaymath}
    \end{document}
{% endlatex %}

Taking the square root of the above, we get the following:

{% latex fig-29 %}
    \begin{document}
    \begin{displaymath}
    F(\omega)
    =
    \sqrt{\displaystyle \frac{e^{- i \mu \omega}}{b^2 \omega^2 + 1}}
    \end{displaymath}
    \end{document}
{% endlatex %}

And now we can take the inverse Fourier transform to get the result:

{% latex fig-30 %}
    \begin{document}
    \begin{displaymath}
    f(x)
    =
    \mathcal{F}^{\,-1\!} \brace2{\lbrace}{\rbrace}{ F(\omega) }
    =
    \frac{1}{2 \pi} \int_{-\infty}^{\infty} F(\omega) \, e^{i \omega x} \, \dderiv \omega
    \end{displaymath}
    \end{document}
{% endlatex %}

As with the previous example, I have yet to figure out how to extract a solution for this integral analytically. Fortunately, however, we can use numerical integration to plot the values and see what the result looks like on a chart:

{% chart fig-31-example-5-pdfunc-fs.svg %}

This seems to have a similar shape to that of the Laplace distribution. But it seems to be taller and skinnier in the middle, and it seems to have flatter and broader tails. Like the previous example, I suspect this takes the form of a generalized normal distribution. But in this case, I think the shape parameter is less than one.

{% accompanying_src_link %}
